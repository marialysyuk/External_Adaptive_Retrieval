{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cf075e2-f267-4310-b469-1b85ca5cd449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "import datasets\n",
    "\n",
    "import argparse\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33cf7824-d1a7-4d50-9428-89525b0c60cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "hf_token = \"hf_token\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "    padding_side='left',\n",
    "    cache_dir=\"/home/jovyan/shares/SR003.nfs2/.cache/models/transformers/\",\n",
    "    token=hf_token,\n",
    "    )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "    cache_dir=\"/home/jovyan/shares/SR003.nfs2/.cache/models/transformers/\",\n",
    "    token=hf_token,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    #attn_implementation=\"flash_attention_2\",\n",
    "    attn_implementation='eager',\n",
    "    ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbc7e095-7d08-4434-9008-3135737d02c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_knowledgability(question):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Answer the following question based\\\n",
    "on your internal knowledge with one or few words. If you are sure the answer is\\\n",
    "accurate and correct, please say '100'. If you are not confident\\\n",
    "with the answer, please range your knowledgability from 0 to 100, say just number. For example, '40'. Question: {question}. Answer:\\\n",
    "    \"},\n",
    "        {\"role\": \"user\", \"content\": question\n",
    "    },\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "492acbd2-60d7-439e-b27b-22e913263a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/adaptive_rag_hotpotqa/train.csv\"\n",
    "test_path = \"data/adaptive_rag_hotpotqa/test.csv\"\n",
    "hf_path = \"AdaRAG/data_hf/external_rag_natural_questions_extra_v4.hf\"\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_hf = datasets.load_from_disk(hf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842db6a8-2598-4c71-8458-1e5b2fb77992",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(df_train))):\n",
    "    if pd.notna(df_train.loc[i, 'bela_base_ents_lbls']):\n",
    "        ents = df_train.loc[i, 'bela_base_ents_lbls'].split(', ')\n",
    "        cur_evals = []\n",
    "        for j in range(len(ents)):\n",
    "            cur_ques = \"question: Do you know a lot about \"+ ents[j] + \"?\"\n",
    "            cur_evals.append(int(generate_paraphrases(cur_ques).split('\\n')[0].replace(\".\", \"\")))\n",
    "        \n",
    "        df_train.loc[i, 'llama_know'] = int(np.mean(cur_evals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b241e6-b01f-42ae-a98a-d0643ef7c5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(df_test))):\n",
    "    if pd.notna(df_test.loc[i, 'bela_base_ents_lbls']):\n",
    "        ents = df_test.loc[i, 'bela_base_ents_lbls'].split(', ')\n",
    "        cur_evals = []\n",
    "        for j in range(len(ents)):\n",
    "            cur_ques = \"question: Do you know a lot about \"+ ents[j] + \"?\"\n",
    "            cur_evals.append(int(generate_paraphrases(cur_ques).split('\\n')[0].replace(\".\", \"\")))\n",
    "        \n",
    "        df_test.loc[i, 'llama_know'] = int(np.mean(cur_evals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5b8a3-69d1-4215-a86f-2927635fcc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['llama_know'] = df_train['llama_know'].fillna(0)\n",
    "df_test['llama_know'] = df_test['llama_know'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87285dab-633c-46c0-9f38-2ba6fcb1c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hf['train'] = df_hf['train'].add_column('llama_know', df_train['llama_know'])\n",
    "df_hf['test'] = df_hf['test'].add_column('llama_know', df_test['llama_know'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef381b22-5e58-4830-bc25-fa2a18f8d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(train_path, index = False)\n",
    "df_test.to_csv(test_path, index = False)\n",
    "df_hf.save_to_disk(hf_path.split('v3.hf')[0]+\"v4.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bd6c8b3e-e5d0-4002-9760-c05be660909a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.98556701030928\n",
      "17.58671587289575\n",
      "20.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(df_trivia['llama_know']))\n",
    "print(np.std(df_trivia['llama_know']))\n",
    "print(np.min(df_trivia['llama_know']))\n",
    "print(np.max(df_trivia['llama_know']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8db23e3e-afd3-4b33-b694-1c00f38819f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.32520325203252\n",
      "22.669504581415506\n",
      "0.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(df_musique['llama_know']))\n",
    "print(np.std(df_musique['llama_know']))\n",
    "print(np.min(df_musique['llama_know']))\n",
    "print(np.max(df_musique['llama_know']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-adaptiverag]",
   "language": "python",
   "name": "conda-env-.mlspace-adaptiverag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
